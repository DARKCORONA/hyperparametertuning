{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "%matplotlib inline\n",
    "\n",
    "# Governing choices for search\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('/home/Bhagwat/home-credit-default-risk/application_train.csv')\n",
    "\n",
    "# Sample 16000 rows (10000 for training, 6000 for testing)\n",
    "features = features.sample(n = 16000, random_state = 42)\n",
    "\n",
    "# Only numeric features\n",
    "features = features.select_dtypes('number')\n",
    "\n",
    "# Extract the labels\n",
    "labels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "features = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "\n",
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 42)\n",
    "\n",
    "print('Train shape: ', train_features.shape)\n",
    "print('Test shape: ', test_features.shape)\n",
    "\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(random_state=50)\n",
    "\n",
    "# Training set\n",
    "train_set = lgb.Dataset(train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default hyperparamters\n",
    "hyperparameters = model.get_params()\n",
    "\n",
    "# Using early stopping to determine number of estimators.\n",
    "del hyperparameters['n_estimators']\n",
    "\n",
    "# Perform cross validation with early stopping\n",
    "cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, metrics = 'auc', \n",
    "           early_stopping_rounds = 100, verbose_eval = False, seed = 42)\n",
    "\n",
    "# Highest score\n",
    "best = cv_results['auc-mean'][-1]\n",
    "\n",
    "# Standard deviation of best score\n",
    "best_std = cv_results['auc-stdv'][-1]\n",
    "\n",
    "print('The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\n",
    "print('The ideal number of iterations was {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(train_features, train_labels)\n",
    "preds = model.predict_proba(test_features)[:, 1]\n",
    "baseline_auc = roc_auc_score(test_labels, preds)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(hyperparameters):\n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Using early stopping to find number of trees trained\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Retrieve the subsample\n",
    "    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type and subsample to top level keys\n",
    "    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "    hyperparameters['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = cv_results['auc-mean'][-1]\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = len(cv_results['auc-mean'])\n",
    "    \n",
    "    # Add the number of estimators to the hyperparameters\n",
    "    hyperparameters['n_estimators'] = n_estimators\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n",
    "    of_connection.close()\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_dist = []\n",
    "\n",
    "# Draw 10000 samples from the learning rate domain\n",
    "for _ in range(10000):\n",
    "    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n",
    "    \n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\n",
    "plt.title('Learning Rate Distribution', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete uniform distribution\n",
    "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
    "num_leaves_dist = []\n",
    "\n",
    "# Sample 10000 times from the number of leaves distribution\n",
    "for _ in range(10000):\n",
    "    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n",
    "    \n",
    "# kdeplot\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\n",
    "plt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting type domain \n",
    "boosting_type = {'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "\n",
    "# Draw a sample\n",
    "hyperparams = sample(boosting_type)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the subsample if present otherwise set to 1.0\n",
    "subsample = hyperparams['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "# Extract the boosting type\n",
    "hyperparams['boosting_type'] = hyperparams['boosting_type']['boosting_type']\n",
    "hyperparams['subsample'] = subsample\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the full space\n",
    "x = sample(space)\n",
    "\n",
    "# Conditional logic to assign top-level keys\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file and open a connection\n",
    "OUT_FILE = 'bayes_test.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Write column names\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "# Test the objective function\n",
    "results = objective(sample(space))\n",
    "print('The cross validation loss = {:.5f}.'.format(results['loss']))\n",
    "print('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "# Create the algorithm\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "# Record results\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file and open a connection\n",
    "OUT_FILE = 'bayes_test.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Write column names\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin\n",
    "\n",
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n",
    "            max_evals = MAX_EVALS)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "trials_dict = sorted(trials.results, key = lambda x: x['loss'])\n",
    "trials_dict[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate(results, name):\n",
    "    \"\"\"Evaluate model on test data using hyperparameters in results\n",
    "       Return dataframe of hyperparameters\"\"\"\n",
    "    \n",
    "    new_results = results.copy()\n",
    "    # String to dictionary\n",
    "    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n",
    "    \n",
    "    # Sort with best values on top\n",
    "    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Print out cross validation high score\n",
    "    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n",
    "    \n",
    "    # Use best hyperparameters to create a model\n",
    "    hyperparameters = new_results.loc[0, 'hyperparameters']\n",
    "    model = lgb.LGBMClassifier(**hyperparameters)\n",
    "    \n",
    "    # Train and make predictions\n",
    "    model.fit(train_features, train_labels)\n",
    "    preds = model.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n",
    "    \n",
    "    # Create dataframe of hyperparameters\n",
    "    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n",
    "\n",
    "    # Iterate through each set of hyperparameters that were evaluated\n",
    "    for i, hyp in enumerate(new_results['hyperparameters']):\n",
    "        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n",
    "                               ignore_index = True)\n",
    "        \n",
    "    # Put the iteration and score in the hyperparameter dataframe\n",
    "    hyp_df['iteration'] = new_results['iteration']\n",
    "    hyp_df['score'] = new_results['score']\n",
    "    \n",
    "    return hyp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_results = evaluate(results, name = 'Bayesian')\n",
    "bayes_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 10\n",
    "\n",
    "# Continue training\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n",
    "            max_evals = MAX_EVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the trial results\n",
    "with open('trials.json', 'w') as f:\n",
    "    f.write(json.dumps(trials_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_results = pd.read_csv('/home/Bhagwat/home-credit-model-tuning/bayesian_trials_1000.csv').sort_values('score', ascending = False).reset_index()\n",
    "random_results = pd.read_csv('/home/Bhagwat/home-credit-model-tuning/random_search_trials_1000.csv').sort_values('score', ascending = False).reset_index()\n",
    "random_results['loss'] = 1 - random_results['score']\n",
    "\n",
    "bayes_params = evaluate(bayes_results, name = 'Bayesian')\n",
    "random_params = evaluate(random_results, name = 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of just scores\n",
    "scores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\n",
    "scores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n",
    "\n",
    "scores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\n",
    "scores['iteration'] = scores['iteration'].astype(np.int32)\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_params = random_params.iloc[random_params['score'].idxmax(), :].copy()\n",
    "best_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of scores over the course of searching\n",
    "sns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\n",
    "plt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\n",
    "plt.scatter(best_random_params['iteration'], best_random_params['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\n",
    "plt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "\n",
    "c = alt.Chart(scores).mark_circle().encode(x = 'iteration', y = alt.Y('ROC AUC', \n",
    "                                                                  scale = alt.Scale(domain = [0.64, 0.74])),\n",
    "                                       color = 'search')\n",
    "c.title = 'Validation ROC AUC vs Iteration'\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "# Density plots of the learning rate distributions \n",
    "sns.kdeplot(learning_rate_dist, label = 'Sampling Distribution', linewidth = 4)\n",
    "sns.kdeplot(random_params['learning_rate'], label = 'Random Search', linewidth = 4)\n",
    "sns.kdeplot(bayes_params['learning_rate'], label = 'Bayes Optimization', linewidth = 4)\n",
    "plt.vlines([best_random_params['learning_rate'], best_bayes_params['learning_rate']],\n",
    "           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\n",
    "plt.legend()\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter\n",
    "for i, hyper in enumerate(random_params.columns):\n",
    "    if hyper not in ['class_weight', 'n_estimators', 'score', 'is_unbalance',\n",
    "                    'boosting_type', 'iteration', 'subsample', 'metric', 'verbose', 'loss', 'learning_rate']:\n",
    "        plt.figure(figsize = (14, 6))\n",
    "        # Plot the random search distribution and the bayes search distribution\n",
    "        if hyper != 'loss':\n",
    "            sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution', linewidth = 4)\n",
    "        sns.kdeplot(random_params[hyper], label = 'Random Search', linewidth = 4)\n",
    "        sns.kdeplot(bayes_params[hyper], label = 'Bayes Optimization', linewidth = 4)\n",
    "        plt.vlines([best_random_params[hyper], best_bayes_params[hyper]],\n",
    "                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\n",
    "        plt.legend(loc = 1)\n",
    "        plt.title('{} Distribution'.format(hyper))\n",
    "        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize = (24, 6))\n",
    "i = 0\n",
    "\n",
    "# Plot of four hyperparameters\n",
    "for i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n",
    "    \n",
    "        # Scatterplot\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize = (24, 6))\n",
    "i = 0\n",
    "\n",
    "# Scatterplot of next three hyperparameters\n",
    "for i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True, sharex = True)\n",
    "\n",
    "# Bar plots of boosting type\n",
    "random_params['boosting_type'].value_counts().plot.bar(ax = axs[0], figsize = (14, 6), color = 'orange', title = 'Random Search Boosting Type')\n",
    "bayes_params['boosting_type'].value_counts().plot.bar(ax = axs[1], figsize = (14, 6), color = 'green', title = 'Bayes Optimization Boosting Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(random_params, width = 500).mark_bar(color = 'orange').encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 400])))\n",
    "text = bars.mark_text(size = 20, align = 'center', baseline = 'bottom').encode(text = 'count()')\n",
    "\n",
    "bars + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(bayes_params, width = 500).mark_bar(color = 'green').encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 800])))\n",
    "text = bars.mark_text(size = 20, align = 'center', baseline = 'bottom').encode(text = 'count()')\n",
    "\n",
    "bars + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full dataset\n",
    "train = pd.read_csv('../input/home-credit-simple-featuers/simple_features_train.csv')\n",
    "test = pd.read_csv('../input/home-credit-simple-featuers/simple_features_test.csv')\n",
    "\n",
    "# Extract the test ids and train labels\n",
    "test_ids = test['SK_ID_CURR']\n",
    "train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "\n",
    "train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "test = test.drop(columns = ['SK_ID_CURR'])\n",
    "\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)\n",
    "bayes_results['hyperparameters'] = bayes_results['hyperparameters'].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(train, label = train_labels)\n",
    "\n",
    "hyperparameters = dict(**random_results.loc[0, 'hyperparameters'])\n",
    "del hyperparameters['n_estimators']\n",
    "\n",
    "# Cross validation with n_folds and early stopping\n",
    "cv_results = lgb.cv(hyperparameters, train_set,\n",
    "                    num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS)\n",
    "\n",
    "print('The cross validation score on the full dataset  for Random Search= {:.5f} with std: {:.5f}.'.format(\n",
    "    cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "preds = model.predict_proba(test)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n",
    "submission.to_csv('submission_random_search.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(**bayes_results.loc[0, 'hyperparameters'])\n",
    "del hyperparameters['n_estimators']\n",
    "\n",
    "# Cross validation with n_folds and early stopping\n",
    "cv_results = lgb.cv(hyperparameters, train_set,\n",
    "                    num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS)\n",
    "\n",
    "print('The cross validation score on the full dataset for Bayesian optimization = {:.5f} with std: {:.5f}.'.format(\n",
    "    cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "preds = model.predict_proba(test)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n",
    "submission.to_csv('submission_bayesian_optimization.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
